{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livebench.model.model_adapter import (\n",
    "    get_conversation_template, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "question = '### Instructions: You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests. You will NOT return anything except for the program.\\n### Question:\\nTakahashi, a young baseball enthusiast, has been a very good boy this year, so Santa has decided to give him a bat or a glove, whichever is more expensive.\\nIf a bat costs B yen and a glove costs G yen (B\\\\neq G), which one will Santa give to Takahashi?\\n\\nInput\\n\\nThe input is given from Standard Input in the following format:\\nB G\\n\\nOutput\\n\\nIf Santa gives Takahashi a bat, print Bat; if Santa gives him a glove, print Glove.\\n\\nConstraints\\n\\n\\n- B and G are different integers between 1 and 1000, inclusive.\\n\\nSample Input 1\\n\\n300 100\\n\\nSample Output 1\\n\\nBat\\n\\nThe bat is more expensive than the glove, so Santa will give Takahashi the bat.\\n\\nSample Input 2\\n\\n334 343\\n\\nSample Output 2\\n\\nGlove\\n\\nThe glove is more expensive than the bat, so Santa will give Takahashi the glove.\\n\\n### Format: Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows.\\n```python\\n# YOUR CODE HERE\\n```\\n\\n### Answer: (use the provided format with backticks)\\n\\n'\n",
    "\n",
    "conv = get_conversation_template(model)\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "temperature = 0.7\n",
    "max_tokens = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.to_openai_api_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = conv.to_openai_api_messages()\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tokenizer.apply_chat_template(messages[1:], tokenize=False)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tokenizer.apply_chat_template(messages[:1], tokenize=False)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_base = \"http://localhost:32000/v1\"\n",
    "openai.api_key = \"mykey\"\n",
    "messages = conv.to_openai_api_messages()\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    n=1,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = tokenizer.apply_chat_template(\n",
    "    conv, tokenize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:32000/generate\",\n",
    "    json=dict(\n",
    "        text=messages,\n",
    "        rid=\"0\",\n",
    "        return_logprob=True,\n",
    "        top_logprobs_num=2, \n",
    "        return_text_in_logprobs=True,\n",
    "        sampling_params=dict(\n",
    "            skip_special_tokens=True,\n",
    "            spaces_between_special_tokens=True,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=10,\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_base = \"http://localhost:32000/v1\"\n",
    "openai.api_key = \"mykey\"\n",
    "messages = conv.to_openai_api_messages()\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    n=1,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    "    stream=True,\n",
    "    stream_options=dict(include_usage=True,)\n",
    ")\n",
    "responses = list(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[-1][\"usage\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_logprobs(x):\n",
    "    results = []\n",
    "    for item in x:\n",
    "        results.append(\n",
    "            (item[\"token\"], item[\"logprob\"])\n",
    "        )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logprobs_info(responses):\n",
    "    xxx = [\n",
    "        (\n",
    "            item[\"token\"], item[\"logprob\"], extract_top_logprobs(item[\"top_logprobs\"])\n",
    "        )\n",
    "        for i in range(len(responses))\n",
    "        for item in responses[i][\"choices\"][0][\"logprobs\"][\"content\"]\n",
    "    ]\n",
    "    return xxx\n",
    "\n",
    "def get_logprobs_info(responses):\n",
    "    return extract_logprobs_info(responses[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_logprobs_info(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = response.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logprobs(w):\n",
    "    return [\n",
    "        w['choices'][0]['logprobs']['content'][i][\"logprob\"]\n",
    "        for i in range(len(w['choices'][0]['logprobs']['content']))\n",
    "    ]\n",
    "\n",
    "logprobs = extract_logprobs(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w['choices'][0]['logprobs']['content'][13][\"token\"]\n",
    "w['choices'][0]['logprobs']['content']#[20][\"top_logprobs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livebench.common import get_categories_tasks, load_questions\n",
    "categories, tasks = get_categories_tasks(\"live_bench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"coding\"\n",
    "task_name = \"coding_completion\"\n",
    "release_set = {'2024-07-26', '2024-06-24', '2024-08-31'}\n",
    "# questions = load_questions(categories[category_name], release_set, task_name, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./df_raw.csv\", \"r\") as file:\n",
    "    df = pd.read_csv(file)\n",
    "df = df[df['task'] == task_name] \n",
    "df = df[df['category'] == category_name]\n",
    "df['score'] = df['score'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby([\n",
    "    \"model\", \"task\", \"category\", \"question_id\", \"question_tok_len\",\n",
    "\n",
    "]).agg(\n",
    "    record_count=pd.NamedAgg(column=\"score\", aggfunc=\"count\"),\n",
    "    score_sum=pd.NamedAgg(column=\"score\", aggfunc=\"sum\"),\n",
    "    score_list=pd.NamedAgg(column=\"score\", aggfunc=list),\n",
    "    answer_tok_len_list=pd.NamedAgg(column=\"llm_answer_tok_len\", aggfunc=list),\n",
    ").reset_index()\n",
    "df_grouped['score_sum'] /= 100\n",
    "df_grouped['score_mean'] = df_grouped['score_sum'] / df_grouped['record_count'] * 100\n",
    "df_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n########## All Tasks ##########\")\n",
    "df_1 = df[[\"model\", \"score\", \"task\"]]\n",
    "df_1 = df_1.groupby([\"model\", \"task\"]).mean()\n",
    "df_1 = pd.pivot_table(df_1, index=['model'], values = \"score\", columns=[\"task\"], aggfunc=\"sum\")\n",
    "df_1 = df_1.round(3)\n",
    "# print(df_1.sort_values(by=\"model\")[:60])\n",
    "# df_1.to_csv('all_tasks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sglang as sgl\n",
    "# from sglang import function, set_default_backend, RuntimeEndpoint\n",
    "# from sglang.lang.interpreter import ProgramState\n",
    "\n",
    "# llm = RuntimeEndpoint(\"http://localhost:34000\")\n",
    "# set_default_backend(llm)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "from fastchat.conversation import Conversation, get_conv_template\n",
    "get_conv_template(\"qwen-7b-chat\")\n",
    "question = '### Instructions: You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests. You will NOT return anything except for the program.\\n### Question:\\nTakahashi, a young baseball enthusiast, has been a very good boy this year, so Santa has decided to give him a bat or a glove, whichever is more expensive.\\nIf a bat costs B yen and a glove costs G yen (B\\\\neq G), which one will Santa give to Takahashi?\\n\\nInput\\n\\nThe input is given from Standard Input in the following format:\\nB G\\n\\nOutput\\n\\nIf Santa gives Takahashi a bat, print Bat; if Santa gives him a glove, print Glove.\\n\\nConstraints\\n\\n\\n- B and G are different integers between 1 and 1000, inclusive.\\n\\nSample Input 1\\n\\n300 100\\n\\nSample Output 1\\n\\nBat\\n\\nThe bat is more expensive than the glove, so Santa will give Takahashi the bat.\\n\\nSample Input 2\\n\\n334 343\\n\\nSample Output 2\\n\\nGlove\\n\\nThe glove is more expensive than the bat, so Santa will give Takahashi the glove.\\n\\n### Format: Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows.\\n```python\\n# YOUR CODE HERE\\n```\\n\\n### Answer: (use the provided format with backticks)\\n\\n'\n",
    "\n",
    "conv = get_conv_template(\"qwen-7b-chat\") # get_conversation_template(model)\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "temperature = 0.7\n",
    "max_tokens = 1000\n",
    "\n",
    "conv.to_openai_api_messages()\n",
    "import openai\n",
    "\n",
    "_response = None\n",
    "\n",
    "def callback_func(response):\n",
    "    global _response\n",
    "    _response = response\n",
    "\n",
    "def chat_completion_openai(model, conv, temperature, max_tokens, api_dict=None, \n",
    "                           use_logprobs=True, return_response=False):\n",
    "    if api_dict is not None:\n",
    "        openai.api_base = api_dict[\"api_base\"]\n",
    "        openai.api_key = api_dict[\"api_key\"]\n",
    "    output = None\n",
    "    \n",
    "    messages = conv.to_openai_api_messages()\n",
    "    import sys\n",
    "    import traceback\n",
    "\n",
    "    def tracefunc(frame, event, arg):\n",
    "        if event == \"call\":\n",
    "            print(f\"Calling function: {frame.f_code.co_name} in file {frame.f_code.co_filename}\")\n",
    "            # print(\"Stack trace:\")\n",
    "            # traceback.print_stack(frame)\n",
    "        return tracefunc\n",
    "\n",
    "    # sys.settrace(tracefunc)\n",
    "    sys.settrace(None)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        logprobs=True,\n",
    "        top_logprobs=2,\n",
    "        return_raw_response_callback=callback_func,\n",
    "    )\n",
    "    sys.settrace(None)\n",
    "    return response\n",
    "    # output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    # if return_response:\n",
    "    #     return response\n",
    "    # else:\n",
    "    #     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: <function callback_func at 0x7f27617f3e20>\n",
      "```python\n",
      "# YOUR CODE HERE\n",
      "b, g = map(int, input().split())\n",
      "if b > g:\n",
      "    print(\"Bat\")\n",
      "else:\n",
      "    print(\"Glove\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "resp = chat_completion_openai(model, conv, temperature, max_tokens, api_dict=dict(\n",
    "    api_base=\"http://localhost:34000/v1\",\n",
    "    api_key=\"mykey\",\n",
    "))\n",
    "output = resp.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_headers': {'date': 'Mon, 25 Nov 2024 19:54:49 GMT', 'server': 'uvicorn', 'content-length': '8610', 'content-type': 'application/json'},\n",
       " 'data': {'id': 'f88d1d82d132489a9a18501621d4ef22',\n",
       "  'object': 'chat.completion',\n",
       "  'created': 1732564490,\n",
       "  'model': 'Qwen/Qwen2.5-7B-Instruct',\n",
       "  'choices': [{'index': 0,\n",
       "    'message': {'role': 'assistant',\n",
       "     'content': '```python\\n# YOUR CODE HERE\\nb, g = map(int, input().split())\\nif b > g:\\n    print(\"Bat\")\\nelse:\\n    print(\"Glove\")\\n```'},\n",
       "    'logprobs': {'content': [{'token': '```',\n",
       "       'bytes': [96, 96, 96],\n",
       "       'logprob': -1.1920928244535389e-07,\n",
       "       'top_logprobs': [{'token': '```',\n",
       "         'bytes': [96, 96, 96],\n",
       "         'logprob': -1.1920928244535389e-07},\n",
       "        {'token': ' ```', 'bytes': [32, 96, 96, 96], 'logprob': -16.625}]},\n",
       "      {'token': 'python',\n",
       "       'bytes': [112, 121, 116, 104, 111, 110],\n",
       "       'logprob': -9.059865078597795e-06,\n",
       "       'top_logprobs': [{'token': 'python',\n",
       "         'bytes': [112, 121, 116, 104, 111, 110],\n",
       "         'logprob': -9.059865078597795e-06},\n",
       "        {'token': 'py', 'bytes': [112, 121], 'logprob': -11.625008583068848}]},\n",
       "      {'token': '\\n',\n",
       "       'bytes': [10],\n",
       "       'logprob': -4.768370445162873e-07,\n",
       "       'top_logprobs': [{'token': '\\n',\n",
       "         'bytes': [10],\n",
       "         'logprob': -4.768370445162873e-07},\n",
       "        {'token': '\\n\\n', 'bytes': [10, 10], 'logprob': -15.375}]},\n",
       "      {'token': '#',\n",
       "       'bytes': [35],\n",
       "       'logprob': -0.0001113352773245424,\n",
       "       'top_logprobs': [{'token': '#',\n",
       "         'bytes': [35],\n",
       "         'logprob': -0.0001113352773245424},\n",
       "        {'token': 'B', 'bytes': [66], 'logprob': -9.12511157989502}]},\n",
       "      {'token': ' YOUR',\n",
       "       'bytes': [32, 89, 79, 85, 82],\n",
       "       'logprob': -0.0007832558476366103,\n",
       "       'top_logprobs': [{'token': ' YOUR',\n",
       "         'bytes': [32, 89, 79, 85, 82],\n",
       "         'logprob': -0.0007832558476366103},\n",
       "        {'token': 'YOUR',\n",
       "         'bytes': [89, 79, 85, 82],\n",
       "         'logprob': -7.750783443450928}]},\n",
       "      {'token': ' CODE',\n",
       "       'bytes': [32, 67, 79, 68, 69],\n",
       "       'logprob': -1.9073304429184645e-05,\n",
       "       'top_logprobs': [{'token': ' CODE',\n",
       "         'bytes': [32, 67, 79, 68, 69],\n",
       "         'logprob': -1.9073304429184645e-05},\n",
       "        {'token': '_CODE',\n",
       "         'bytes': [95, 67, 79, 68, 69],\n",
       "         'logprob': -11.125019073486328}]},\n",
       "      {'token': ' HERE',\n",
       "       'bytes': [32, 72, 69, 82, 69],\n",
       "       'logprob': -3.671578815556131e-05,\n",
       "       'top_logprobs': [{'token': ' HERE',\n",
       "         'bytes': [32, 72, 69, 82, 69],\n",
       "         'logprob': -3.671578815556131e-05},\n",
       "        {'token': 'HERE',\n",
       "         'bytes': [72, 69, 82, 69],\n",
       "         'logprob': -10.375036239624023}]},\n",
       "      {'token': '\\n',\n",
       "       'bytes': [10],\n",
       "       'logprob': -1.1920928244535389e-07,\n",
       "       'top_logprobs': [{'token': '\\n',\n",
       "         'bytes': [10],\n",
       "         'logprob': -1.1920928244535389e-07},\n",
       "        {'token': '\\n\\n', 'bytes': [10, 10], 'logprob': -16.625}]},\n",
       "      {'token': 'b',\n",
       "       'bytes': [98],\n",
       "       'logprob': -0.6037272810935974,\n",
       "       'top_logprobs': [{'token': 'b',\n",
       "         'bytes': [98],\n",
       "         'logprob': -0.6037272810935974},\n",
       "        {'token': 'B', 'bytes': [66], 'logprob': -0.8537272810935974}]},\n",
       "      {'token': ',',\n",
       "       'bytes': [44],\n",
       "       'logprob': -8.940297266235575e-05,\n",
       "       'top_logprobs': [{'token': ',',\n",
       "         'bytes': [44],\n",
       "         'logprob': -8.940297266235575e-05},\n",
       "        {'token': ' =', 'bytes': [32, 61], 'logprob': -9.625089645385742}]},\n",
       "      {'token': ' g',\n",
       "       'bytes': [32, 103],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': ' g', 'bytes': [32, 103], 'logprob': 0.0},\n",
       "        {'token': '.g', 'bytes': [46, 103], 'logprob': -22.125}]},\n",
       "      {'token': ' =',\n",
       "       'bytes': [32, 61],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': ' =', 'bytes': [32, 61], 'logprob': 0.0},\n",
       "        {'token': '=', 'bytes': [61], 'logprob': -23.375}]},\n",
       "      {'token': ' map',\n",
       "       'bytes': [32, 109, 97, 112],\n",
       "       'logprob': -1.5258672647178173e-05,\n",
       "       'top_logprobs': [{'token': ' map',\n",
       "         'bytes': [32, 109, 97, 112],\n",
       "         'logprob': -1.5258672647178173e-05},\n",
       "        {'token': 'map',\n",
       "         'bytes': [109, 97, 112],\n",
       "         'logprob': -11.750015258789062}]},\n",
       "      {'token': '(int',\n",
       "       'bytes': [40, 105, 110, 116],\n",
       "       'logprob': -4.768370445162873e-07,\n",
       "       'top_logprobs': [{'token': '(int',\n",
       "         'bytes': [40, 105, 110, 116],\n",
       "         'logprob': -4.768370445162873e-07},\n",
       "        {'token': '=int', 'bytes': [61, 105, 110, 116], 'logprob': -15.125}]},\n",
       "      {'token': ',',\n",
       "       'bytes': [44],\n",
       "       'logprob': -2.3841855067985307e-07,\n",
       "       'top_logprobs': [{'token': ',',\n",
       "         'bytes': [44],\n",
       "         'logprob': -2.3841855067985307e-07},\n",
       "        {'token': ',input',\n",
       "         'bytes': [44, 105, 110, 112, 117, 116],\n",
       "         'logprob': -15.25}]},\n",
       "      {'token': ' input',\n",
       "       'bytes': [32, 105, 110, 112, 117, 116],\n",
       "       'logprob': -3.2186455882765586e-06,\n",
       "       'top_logprobs': [{'token': ' input',\n",
       "         'bytes': [32, 105, 110, 112, 117, 116],\n",
       "         'logprob': -3.2186455882765586e-06},\n",
       "        {'token': '(input',\n",
       "         'bytes': [40, 105, 110, 112, 117, 116],\n",
       "         'logprob': -13.12500286102295}]},\n",
       "      {'token': '().',\n",
       "       'bytes': [40, 41, 46],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': '().',\n",
       "         'bytes': [40, 41, 46],\n",
       "         'logprob': 0.0},\n",
       "        {'token': ' ().', 'bytes': [32, 40, 41, 46], 'logprob': -18.0}]},\n",
       "      {'token': 'split',\n",
       "       'bytes': [115, 112, 108, 105, 116],\n",
       "       'logprob': -7.045020902296528e-05,\n",
       "       'top_logprobs': [{'token': 'split',\n",
       "         'bytes': [115, 112, 108, 105, 116],\n",
       "         'logprob': -7.045020902296528e-05},\n",
       "        {'token': 'strip',\n",
       "         'bytes': [115, 116, 114, 105, 112],\n",
       "         'logprob': -9.625070571899414}]},\n",
       "      {'token': '())\\n',\n",
       "       'bytes': [40, 41, 41, 10],\n",
       "       'logprob': -2.1576648578047752e-05,\n",
       "       'top_logprobs': [{'token': '())\\n',\n",
       "         'bytes': [40, 41, 41, 10],\n",
       "         'logprob': -2.1576648578047752e-05},\n",
       "        {'token': '())\\n\\n',\n",
       "         'bytes': [40, 41, 41, 10, 10],\n",
       "         'logprob': -10.875021934509277}]},\n",
       "      {'token': 'if',\n",
       "       'bytes': [105, 102],\n",
       "       'logprob': -0.0002964295563288033,\n",
       "       'top_logprobs': [{'token': 'if',\n",
       "         'bytes': [105, 102],\n",
       "         'logprob': -0.0002964295563288033},\n",
       "        {'token': 'print',\n",
       "         'bytes': [112, 114, 105, 110, 116],\n",
       "         'logprob': -8.125296592712402}]},\n",
       "      {'token': ' b',\n",
       "       'bytes': [32, 98],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': ' b', 'bytes': [32, 98], 'logprob': 0.0},\n",
       "        {'token': ' B', 'bytes': [32, 66], 'logprob': -19.0}]},\n",
       "      {'token': ' >',\n",
       "       'bytes': [32, 62],\n",
       "       'logprob': -6.9141146923357155e-06,\n",
       "       'top_logprobs': [{'token': ' >',\n",
       "         'bytes': [32, 62],\n",
       "         'logprob': -6.9141146923357155e-06},\n",
       "        {'token': ' <', 'bytes': [32, 60], 'logprob': -11.875006675720215}]},\n",
       "      {'token': ' g',\n",
       "       'bytes': [32, 103],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': ' g', 'bytes': [32, 103], 'logprob': 0.0},\n",
       "        {'token': '\\tg', 'bytes': [9, 103], 'logprob': -22.625}]},\n",
       "      {'token': ':\\n',\n",
       "       'bytes': [58, 10],\n",
       "       'logprob': -1.5497195136049413e-06,\n",
       "       'top_logprobs': [{'token': ':\\n',\n",
       "         'bytes': [58, 10],\n",
       "         'logprob': -1.5497195136049413e-06},\n",
       "        {'token': ':', 'bytes': [58], 'logprob': -13.375001907348633}]},\n",
       "      {'token': '   ',\n",
       "       'bytes': [32, 32, 32],\n",
       "       'logprob': -0.0007102350937202573,\n",
       "       'top_logprobs': [{'token': '   ',\n",
       "         'bytes': [32, 32, 32],\n",
       "         'logprob': -0.0007102350937202573},\n",
       "        {'token': ' ', 'bytes': [32], 'logprob': -7.2507100105285645}]},\n",
       "      {'token': ' print',\n",
       "       'bytes': [32, 112, 114, 105, 110, 116],\n",
       "       'logprob': -1.1920928244535389e-07,\n",
       "       'top_logprobs': [{'token': ' print',\n",
       "         'bytes': [32, 112, 114, 105, 110, 116],\n",
       "         'logprob': -1.1920928244535389e-07},\n",
       "        {'token': 'print',\n",
       "         'bytes': [112, 114, 105, 110, 116],\n",
       "         'logprob': -15.875}]},\n",
       "      {'token': '(\"',\n",
       "       'bytes': [40, 34],\n",
       "       'logprob': -0.4740769863128662,\n",
       "       'top_logprobs': [{'token': '(\"',\n",
       "         'bytes': [40, 34],\n",
       "         'logprob': -0.4740769863128662},\n",
       "        {'token': \"('\", 'bytes': [40, 39], 'logprob': -0.9740769863128662}]},\n",
       "      {'token': 'Bat',\n",
       "       'bytes': [66, 97, 116],\n",
       "       'logprob': -0.003739984007552266,\n",
       "       'top_logprobs': [{'token': 'Bat',\n",
       "         'bytes': [66, 97, 116],\n",
       "         'logprob': -0.003739984007552266},\n",
       "        {'token': 'G', 'bytes': [71], 'logprob': -5.628739833831787}]},\n",
       "      {'token': '\")\\n',\n",
       "       'bytes': [34, 41, 10],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': '\")\\n',\n",
       "         'bytes': [34, 41, 10],\n",
       "         'logprob': 0.0},\n",
       "        {'token': \"')\\n\", 'bytes': [39, 41, 10], 'logprob': -17.125}]},\n",
       "      {'token': 'else',\n",
       "       'bytes': [101, 108, 115, 101],\n",
       "       'logprob': -0.00015841660206206143,\n",
       "       'top_logprobs': [{'token': 'else',\n",
       "         'bytes': [101, 108, 115, 101],\n",
       "         'logprob': -0.00015841660206206143},\n",
       "        {'token': 'elif',\n",
       "         'bytes': [101, 108, 105, 102],\n",
       "         'logprob': -8.750158309936523}]},\n",
       "      {'token': ':\\n',\n",
       "       'bytes': [58, 10],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': ':\\n', 'bytes': [58, 10], 'logprob': 0.0},\n",
       "        {'token': ':\\r\\n', 'bytes': [58, 13, 10], 'logprob': -20.5}]},\n",
       "      {'token': '   ',\n",
       "       'bytes': [32, 32, 32],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': '   ',\n",
       "         'bytes': [32, 32, 32],\n",
       "         'logprob': 0.0},\n",
       "        {'token': '  ', 'bytes': [32, 32], 'logprob': -20.0}]},\n",
       "      {'token': ' print',\n",
       "       'bytes': [32, 112, 114, 105, 110, 116],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': ' print',\n",
       "         'bytes': [32, 112, 114, 105, 110, 116],\n",
       "         'logprob': 0.0},\n",
       "        {'token': 'print',\n",
       "         'bytes': [112, 114, 105, 110, 116],\n",
       "         'logprob': -19.75}]},\n",
       "      {'token': '(\"',\n",
       "       'bytes': [40, 34],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': '(\"', 'bytes': [40, 34], 'logprob': 0.0},\n",
       "        {'token': '=\"', 'bytes': [61, 34], 'logprob': -21.75}]},\n",
       "      {'token': 'G',\n",
       "       'bytes': [71],\n",
       "       'logprob': 0.0,\n",
       "       'top_logprobs': [{'token': 'G', 'bytes': [71], 'logprob': 0.0},\n",
       "        {'token': ' Glo', 'bytes': [32, 71, 108, 111], 'logprob': -18.375}]},\n",
       "      {'token': 'love',\n",
       "       'bytes': [108, 111, 118, 101],\n",
       "       'logprob': -0.0002637753786984831,\n",
       "       'top_logprobs': [{'token': 'love',\n",
       "         'bytes': [108, 111, 118, 101],\n",
       "         'logprob': -0.0002637753786984831},\n",
       "        {'token': 'lo', 'bytes': [108, 111], 'logprob': -8.250264167785645}]},\n",
       "      {'token': '\")\\n',\n",
       "       'bytes': [34, 41, 10],\n",
       "       'logprob': -2.3841855067985307e-07,\n",
       "       'top_logprobs': [{'token': '\")\\n',\n",
       "         'bytes': [34, 41, 10],\n",
       "         'logprob': -2.3841855067985307e-07},\n",
       "        {'token': \"')\\n\", 'bytes': [39, 41, 10], 'logprob': -15.5}]},\n",
       "      {'token': '```',\n",
       "       'bytes': [96, 96, 96],\n",
       "       'logprob': -1.1920928244535389e-07,\n",
       "       'top_logprobs': [{'token': '```',\n",
       "         'bytes': [96, 96, 96],\n",
       "         'logprob': -1.1920928244535389e-07},\n",
       "        {'token': ' ```', 'bytes': [32, 96, 96, 96], 'logprob': -17.0}]},\n",
       "      {'token': '<|im_end|>',\n",
       "       'bytes': [60, 124, 105, 109, 95, 101, 110, 100, 124, 62],\n",
       "       'logprob': -2.5748875486897305e-05,\n",
       "       'top_logprobs': [{'token': '<|im_end|>',\n",
       "         'bytes': [60, 124, 105, 109, 95, 101, 110, 100, 124, 62],\n",
       "         'logprob': -2.5748875486897305e-05},\n",
       "        {'token': '<|im_start|>',\n",
       "         'bytes': [60, 124, 105, 109, 95, 115, 116, 97, 114, 116, 124, 62],\n",
       "         'logprob': -12.437525749206543}]}]},\n",
       "    'finish_reason': 'stop'}],\n",
       "  'usage': {'prompt_tokens': 334,\n",
       "   'total_tokens': 373,\n",
       "   'completion_tokens': 39,\n",
       "   'prompt_tokens_details': None}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang-private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
